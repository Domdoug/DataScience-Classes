{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML - 7. Pipelines_SalvandoModelos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKU4syAPPUxT",
        "colab_type": "text"
      },
      "source": [
        "# Especialização em Ciência de Dados - PUC-Rio\n",
        "# Machine Learning\n",
        "## Automatização de Fluxos com Pipelines / Salvando e carregando modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZuQVWj5fcD",
        "colab_type": "text"
      },
      "source": [
        "# Automatização de Fluxos com Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFFNs_iLPYbm",
        "colab_type": "text"
      },
      "source": [
        "Existem fluxos de trabalho padrão no aprendizado de máquina aplicado. O scikit-learn do Python fornece um utilitário Pipeline para ajudar a automatizar os fluxos de trabalho de aprendizado de máquina. Os pipelines permitem que uma sequência linear de transformações de dados seja encadeada, culminando em um processo de modelagem que pode ser avaliado.\n",
        "\n",
        "O objetivo é garantir que todas as etapas do pipeline sejam restritas aos dados disponíveis para a avaliação, como o conjunto de dados de treinamento ou cada fold do procedimento de validação cruzada. \n",
        "\n",
        "Você pode aprender mais sobre pipelines no scikit-learn lendo a seção Pipeline do guia do usuário e na documentação da API para as classes Pipeline e FeatureUnion e o módulo de pipeline:\n",
        "* http://scikit-learn.org/stable/modules/pipeline.html \n",
        "* http://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBZAI-4xZcrG",
        "colab_type": "text"
      },
      "source": [
        "## Importação dos dados\n",
        "Usaremos a mesma base de dados que trabalhamos anteriormente, o dataset Pima Indians Diabetes. O trecho de código a seguir é idêntico ao que fizemos em laboratórios anteriores e serve apenas para importar os dados e separá-los em entradas (X) e saídas (Y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT2KnwJaZcxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carrega arquivo csv usando Pandas usando uma URL\n",
        "\n",
        "# Importa todo o pacote Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Importa a função datasets\n",
        "from sklearn import datasets\n",
        "\n",
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "# Informa o cabeçalho das colunas\n",
        "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "# Lê o arquivo utilizando as colunas informadas\n",
        "dataset = pd.read_csv(url, names=colunas, skiprows=0, delimiter=',')\n",
        "\n",
        "# Pega apenas os dados do dataset e guardando em um array\n",
        "array = dataset.values\n",
        "\n",
        "# Separa o array em variáveis preditoras (X) e variável target (Y)\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ODlk-lPg_9",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline de preparação e modelagem de dados\n",
        "\n",
        "Uma armadilha comum no aprendizado de máquina aplicado é o \"vazamento\" de dados do conjunto de dados de treinamento para o conjunto de dados de teste. Para evitar essa armadilha, precisamos de uma forte separação de treinamento e teste, incluindo a preparação de dados. Na preparação de dados é fácil \"vazar\" o conhecimento de todo o conjunto de dados de treinamento para o algoritmo. Por exemplo, preparar os dados usando normalização ou padronização em todo o conjunto de dados de treinamento antes do aprendizado não seria recomendado, porque as partições de treinamento seriam influenciados pela escala dos dados da partição do conjunto de testes.\n",
        "\n",
        "Os pipelines garantem que a preparação de dados seja restrita a cada fold da validação cruzada. Neste exemplo, o pipeline é definido com duas etapas:\n",
        "\n",
        "1. Padronização dos dados.\n",
        "2. Aprendizado de um modelo.\n",
        "\n",
        "O pipeline é então avaliado usando a validação cruzada 10-fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWpPl0jBPo56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criação de um pipeline para padronizar os dados e criar um modelo\n",
        "\n",
        "# Imports\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Criação do pipeline com apenas 1 modelo\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
        "model = Pipeline(estimators)\n",
        "\n",
        "# Avaliação do pipeline\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8iVetsaP1UY",
        "colab_type": "text"
      },
      "source": [
        "Observe como criamos uma lista Python de etapas fornecidas ao Pipeline para o processamento dos dados. Observe também como o próprio pipeline é tratado como um estimador e é avaliado na íntegra pelo procedimento de validação cruzada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKAGxV_9P3RV",
        "colab_type": "text"
      },
      "source": [
        "## Extração de features e pipeline de modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2K6IEewP3Zw",
        "colab_type": "text"
      },
      "source": [
        "A extração de features é outro procedimento suscetível a vazamento de dados. Como a preparação de dados, os procedimentos de extração de features devem ser restritos aos dados do conjunto de dados de treinamento. O pipeline fornece uma ferramenta útil chamada FeatureUnion, que permite combinar os resultados de vários procedimentos de seleção e extração de features em um conjunto de dados maior no qual um modelo pode ser treinado. Toda a extração e união de features ocorre em cada fold do procedimento de validação cruzada. Neste exemplo, o pipeline é definido com quatro etapas:\n",
        "\n",
        "1. Extração de features com análise de componentes principais (3 features).\n",
        "2. Extração de features com seleção estatística (6 features).\n",
        "3. União de features.\n",
        "4. Aprendizado de um modelo.\n",
        "\n",
        "O pipeline é então avaliado usando a validação cruzada 10-fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnTRSuOJP-8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criação de um pipeline para extrair features dos dados e criar um modelo\n",
        "\n",
        "# Imports\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "# Cria a união de features\n",
        "features = []\n",
        "features.append(('pca', PCA(n_components=3)))\n",
        "features.append(('select_best', SelectKBest(k=6)))\n",
        "feature_union = FeatureUnion(features)\n",
        "\n",
        "# Cria o pipeline com apenas 1 modelo\n",
        "estimators = []\n",
        "estimators.append(('feature_union', feature_union)) \n",
        "estimators.append(('logistic', LogisticRegression(solver='liblinear')))\n",
        "model = Pipeline(estimators)\n",
        "\n",
        "# Avaliação do pipeline\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLoEuytBQBmQ",
        "colab_type": "text"
      },
      "source": [
        "Observe como o FeatureUnion é seu próprio pipeline que, por sua vez, é uma única etapa no pipeline final usado para alimentar a regressão logística. Isso pode fazer você pensar em como começar a incorporar pipelines dentro de pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHYHtSXF4szs",
        "colab_type": "text"
      },
      "source": [
        "# Salvando e Carregando Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa3eafljmpCz",
        "colab_type": "text"
      },
      "source": [
        "## Pickle\n",
        "https://docs.python.org/2/library/pickle.html\n",
        "\n",
        "É a maneira padrão de serializar objetos em Python, sendo possível serializar modelos de aprendizado de máquina e salvar o formato serializado em um arquivo. Posteriormente, é possível pode carregar esse arquivo para desserializar o modelo e usá-lo para fazer novas previsões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtyMRxq1mpJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "\n",
        "# Divide os dados em treino e teste\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
        "\n",
        "# Cria o modelo\n",
        "modelo = LogisticRegression(solver='liblinear') \n",
        "\n",
        "# Treina o modelo\n",
        "modelo.fit(X_train, Y_train)\n",
        "\n",
        "# Salva o modelo no disco\n",
        "filename = 'finalized_model.sav'\n",
        "dump(modelo, open(filename, 'wb'))\n",
        "\n",
        "# Algum tempo depois...\n",
        "# Carrega o modelo no disco\n",
        "loaded_model = load(open(filename, 'rb')) \n",
        "result = loaded_model.score(X_test, Y_test) \n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PjWy_HympQu",
        "colab_type": "text"
      },
      "source": [
        "## Joblib\n",
        "\n",
        "https://pypi.python.org/pypi/joblib\n",
        "https://pythonhosted.org/joblib/generated/joblib.dump.html\n",
        "\n",
        "A biblioteca Joblib faz parte do ecossistema SciPy e fornece utilitários para pipeline de trabalhos em Python. Fornece utilitários para salvar e carregar objetos Python que fazem uso eficiente das estruturas de dados do NumPy. Isso pode ser útil para alguns algoritmos de aprendizado de máquina que exigem muitos parâmetros ou armazenam todo o conjunto de dados (por exemplo, KNN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wEIq42PmpWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.externals.joblib import dump\n",
        "from sklearn.externals.joblib import load\n",
        "\n",
        "# Divide os dados em treino e teste\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7) \n",
        "\n",
        "# Cria o modelo\n",
        "modelo = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Treina o modelo\n",
        "modelo.fit(X_train, Y_train)\n",
        "\n",
        "# Salva o modelo no disco\n",
        "filename = 'finalized_model.sav' \n",
        "dump(modelo, filename)\n",
        "\n",
        "# Algum tempo depois...\n",
        "# Carrega o modelo no disco\n",
        "loaded_model = load(filename)\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}