{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_9.Projeto completo de Regressão.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4-Agy2vHp3b",
        "colab_type": "text"
      },
      "source": [
        "# Especialização em Ciência de Dados - PUC-Rio\n",
        "# Machine Learning\n",
        "# Projeto completo de Regressão\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-olyxQ7ZFiSw",
        "colab_type": "text"
      },
      "source": [
        "## 1. Definição do Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdQodjGgLCD0",
        "colab_type": "text"
      },
      "source": [
        "Para este projeto, investigaremos o dataset Boston House Price, um dataset para problemas de regressão em que todos os atributos são numéricos.\n",
        "\n",
        "Cada registro no banco de dados descreve um subúrbio ou cidade de Boston. Os dados foram retirados da Área Estatística Metropolitana Padrão de Boston (SMSA) em 1970. Os atributos são definidos da seguinte forma (retirado do UCI Machine Learning Repository):\n",
        "\n",
        "1. CRIM: Taxa de criminalidade per capita por cidade\n",
        "2. ZN: Proporção de terrenos residenciais divididos em lotes com mais de 25.000 pés quadrados\n",
        "3. INDUS: Proporção de acres não comerciais por cidade\n",
        "4. CHAS: Variável fictícia CHAS Charles River (= 1 se o trecho limita o rio; 0 caso contrário)\n",
        "5. NOX: Concentração de óxidos nítricos (partes por 10 milhões)\n",
        "6. RM: número médio de quartos por habitação\n",
        "7. AGE: proporção de unidades ocupadas construídas antes de 1940\n",
        "8. DIS: Distâncias ponderadas para cinco centros de emprego em Boston\n",
        "9. RAD: Índice de acessibilidade às rodovias radiais\n",
        "10. TAX: Taxa de imposto sobre o valor total da propriedade por 10 mil dólares\n",
        "11. PTRATIO: Proporção de alunos por professor por cidade\n",
        "12. B: 1000(Bk - 0.63)^2 onde Bk é a proporção de negros por cidade\n",
        "13. LSTAT: % do menor status da população\n",
        "14. MEDV: Valor médio das casas ocupadas pelos proprietários em mil dólares\n",
        "Podemos ver que os atributos de entrada têm uma mistura de unidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2EmWS5KH4Lz",
        "colab_type": "text"
      },
      "source": [
        "## 2. Carga de Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUGPuR5RHkR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy\n",
        "from numpy import arange\n",
        "from matplotlib import pyplot\n",
        "from matplotlib import cm\n",
        "from pandas import read_csv\n",
        "from pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU6rSAAXH79N",
        "colab_type": "text"
      },
      "source": [
        "Faça download do dataset no site do repositório do UCI Machine Learning (https://archive.ics.uci.edu/ml/machine-learning-databases/housing/), salvando no diretório de trabalho local com o nome de arquivo housing.csv.\n",
        "\n",
        "Iremos especificar os nomes abreviados para cada atributo, para podermos referenciá-los claramente depois. Os atributos são delimitados por espaços em branco, em vez de vírgulas neste arquivo, e indicamos isso para ler a função csv () através do argumento delim_whitespace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd60hGIwH8DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carga do dataset\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "'B', 'LSTAT', 'MEDV']\n",
        "dataset = read_csv(filename, delim_whitespace=True, names=names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9qWZGOJIAHD",
        "colab_type": "text"
      },
      "source": [
        "## 3. Análise de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-PS3DYSMNjm",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Estatísticas Descritivas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0BmLndDIAOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensões do dataset\n",
        "print(dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S1MKWzaIAbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tipos de cada atributo\n",
        "print(dataset.dtypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlTFxFVJIAh1",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que todos os atributos são numéricos, principalmente valores reais (float) e alguns foram interpretados como números inteiros (int)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_HGtXIIAoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # head\n",
        "print(dataset.head(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6jhZX-OIQXs",
        "colab_type": "text"
      },
      "source": [
        "Podemos confirmar que as escalas dos atributos são muito diferentes devido às diferentes unidades. Utilizaremos algumas transformações mais tarde. Vamos resumir a distribuição de cada atributo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo1p8ReuIQgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sumário estatístico\n",
        "set_option('precision', 1) \n",
        "print(dataset.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G9GiRWBIQoa",
        "colab_type": "text"
      },
      "source": [
        "Agora temos uma sensação melhor de como os atributos são diferentes. Os valores mínimo e máximo, bem como as médias, variam muito. Provavelmente, obteremos melhores resultados redimensionando os dados de alguma forma. Agora, vamos dar uma olhada na correlação entre todos os atributos numéricos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2hWHAYVIQvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correlação\n",
        "set_option('precision', 2) \n",
        "print(dataset.corr(method='pearson'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcdRZEqAIQ2O",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que muitos dos atributos têm uma forte correlação (por exemplo,> 0,70 ou <-0,70). Por exemplo:\n",
        "* NOX e INDUS com 0,77.\n",
        "* DIS e INDUS com -0,71.\n",
        "* IMPOSTO e INDUS com 0,72.\n",
        "* IDADE e NOX com 0,73.\n",
        "* DIS e NOX com -0,78.\n",
        "Também parece que o LSTAT tem uma boa correlação negativa com a variável de saída MEDV com um valor de -0,74."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ADShmcUIRDx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 3.2. Visualizações Unimodais\n",
        "Vamos agora gerar visualizações dos atributos individualmente.\n",
        "\n",
        "Primeiro, vamos gerar os histogramas de cada atributo para ter uma ideia das distribuições de dados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ_p1quCIRKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogramas\n",
        "dataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12,8))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV3MC-UyIRRe",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que alguns atributos parecem ter uma distribuição exponencial, como CRIM, ZN, AGE e B. Podemos ver que outros parecem ter uma distribuição bimodal, como RAD e TAX. Vejamos as mesmas distribuições usando gráficos de densidade que as suavizam um pouco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ORXYUb7IRX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# density plots\n",
        "dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False, legend=False, fontsize=1, figsize=(12,8))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIsAbHTNIRet",
        "colab_type": "text"
      },
      "source": [
        "Isto acrescente mais evidências à nossa suspeita sobre possíveis distribuições exponenciais e bimodais. Também parece que NOX, RM e LSTAT podem ser distribuições Gaussianas distorcidas, informação que pode ser útil posteriormente nas transformações. Vamos analisar os dados com boxplots de cada atributo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edeUEyeVIRk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# boxplots\n",
        "dataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=8, figsize=(12,8))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B47BAdt-LU31",
        "colab_type": "text"
      },
      "source": [
        "Isso ajuda a apontar tanto a distorção em muitas distribuições, quanto os dados que parecem ser outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb3UjFDfIRr1",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Visualizações Multimodais\n",
        "Vamos visualizar algumas interações entre variáveis, começando com uma matriz de dispersão (scatter plot matrix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Mgq5mIIRyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scatter plot matrix\n",
        "scatter_matrix(dataset, figsize=(12,8))\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-cv58qSIR5J",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que alguns dos atributos com correlação mais alta mostram boa estrutura em seu relacionamento. Vamos visualizar também as correlações entre os atributos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KWAZ7_QIR_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matriz de Correlação\n",
        "fig = pyplot.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none', cmap=cm.get_cmap('RdBu')) \n",
        "fig.colorbar(cax)\n",
        "ticks = numpy.arange(0,14,1)\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_xticklabels(names)\n",
        "ax.set_yticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JH8EyeLYtE",
        "colab_type": "text"
      },
      "source": [
        "A cor azul escuro mostra correlação positiva, enquanto a cor vermelho escuro mostra correlação negativa. Também podemos ver alguns vermelhos e azuis escuros que sugerem candidatos à remoção para melhorar a precisão dos modelos posteriormente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKxQd4wOJA_y",
        "colab_type": "text"
      },
      "source": [
        "### 3.4. Resumo das Ideias\n",
        "Há muita estrutura neste conjunto de dados. Precisamos pensar nas transformações que poderíamos usar posteriormente para melhor expor a estrutura que, por sua vez, pode melhorar a precisão da modelagem. Até agora, valeria a pena tentar:\n",
        "* Seleção de recursos e remoção dos atributos mais correlacionados.\n",
        "* Normalização do conjunto de dados para reduzir o efeito de diferentes escalas.\n",
        "* Padronização do conjunto de dados para reduzir os efeitos de diferentes distribuições.\n",
        "\n",
        "Também poderíamos explorar a possibilidade de discretização dos dados, o que pode melhorar a precisão dos algoritmos de árvore de decisão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n3Ufe1oLn2N",
        "colab_type": "text"
      },
      "source": [
        "## 4. Pré-Processamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qEcHWwPMN1y",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. Conjunto de validação\n",
        "\n",
        "É uma boa prática usar um conjunto de validação (na literatura também chamado de conjunto de testes), uma amostra dos dados que não será usada para a modelagem, mas somente no fim do projeto para confirmar a precisão do  modelo final. É um teste  que podemos usar para verificar se erramos e para nos dar confiança nas estimativas em dados não vistos. Usaremos 80% do conjunto de dados para modelagem e guardaremos 20% para validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4YqxtCQJBIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separação em conjuntos de treino e validação\n",
        "array = dataset.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "validation_size = 0.20\n",
        "seed = 7\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
        "    test_size=validation_size, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsqOkfIKMXrh",
        "colab_type": "text"
      },
      "source": [
        "## 5. Modelos de Regressão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0sEWM0YJBRn",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Criação e avaliação de modelos: linha base\n",
        "\n",
        "Não sabemos quais modelos performarão bem neste conjunto de dados. Para testar, usaremos a validação cruzada 10-fold e avaliaremos os modelos usando a métrica de Mean Squared Error (MSE). O MSE dará uma idéia geral de quão erradas todas as previsões são (0 é perfeito)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWa8QRlNJBZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parâmetros\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "scoring = 'neg_mean_squared_error'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doenRQPBJBi2",
        "colab_type": "text"
      },
      "source": [
        "Vamos criar uma linha base de desempenho para esse problema e verificar vários modelos diferentes com suas configurações padrão:\n",
        "* **Algortimos Linear:** Linear Regression (LR), Lasso Regression (LASSO) e ElasticNet (EN).\n",
        "* **Algortimos Não-lineares :** Classification and Regression Trees (CART), Support Vector Regression (SVR) e k-Nearest Neighbors (KNN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig1zOlh9JBq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criação dos modelos\n",
        "models = []\n",
        "models.append(('LR', LinearRegression())) \n",
        "models.append(('LASSO', Lasso())) \n",
        "models.append(('EN', ElasticNet())) \n",
        "models.append(('KNN', KNeighborsRegressor())) \n",
        "models.append(('CART', DecisionTreeRegressor())) \n",
        "models.append(('SVR', SVR(gamma=\"auto\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7jTKn7uJBx4",
        "colab_type": "text"
      },
      "source": [
        "Agora vamos comparar os modelos, exibindo o MSE médio e o desvio padrão de cada um:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bZN9R7lJB4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avaliação dos modelos\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0WuSRfWJB-A",
        "colab_type": "text"
      },
      "source": [
        "A LR tem o MSE mais baixo, seguido de perto pela CART, mas estes são apenas valores médios de MSE, sendo também prudente observar a distribuição dos resultados de cada fold da validação cruzada. Faremos isso com os whisker plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_K1S-agJCD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparação dos modelos\n",
        "fig = pyplot.figure() \n",
        "fig.suptitle('Comparação dos modelos') \n",
        "ax = fig.add_subplot(111) \n",
        "pyplot.boxplot(results) \n",
        "ax.set_xticklabels(names) \n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjkvVl4uLhmz",
        "colab_type": "text"
      },
      "source": [
        "As diferentes escalas dos dados provavelmente estão prejudicando a habilidade de todos os algoritmos e talvez mais ainda para SVR e KNN. A seguir, repetiremos este processo usando uma cópia padronizada do conjunto de dados de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IArQMiDJCJq",
        "colab_type": "text"
      },
      "source": [
        "### 5.2. Criação e avaliação de modelos: dados padronizados\n",
        "\n",
        "Como suspeitamos que as diferentes distribuições dos dados brutos possam impactar negativamente a habilidade de alguns modelos, vamos agora utilizar cópia padronizada do dataset. Os dados serão transformados de modo que cada atributo tenha média 0 e um desvio padrão 1. \n",
        "\n",
        "Para evitar o \"vazamento de dados\" na transformação, vamos usar pipelines que padronizam os dados e constroem o modelo para cada fold de teste de validação cruzada. Dessa forma, podemos obter uma estimativa justa de como cada modelo com dados padronizados pode funcionar com dados não vistos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qRW58xrJCOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padronização do dataset\n",
        "pipelines = []\n",
        "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\n",
        "LinearRegression())])))\n",
        "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO',\n",
        "    Lasso())])))\n",
        "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
        "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
        "pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())]))) \n",
        "results = []\n",
        "names = []\n",
        "for name, model in pipelines:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH2gLMdJCUU",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que a padronização afetou o KNN, diminuindo o erro para abaixo dos outros modelos. O erro do SVR também diminuiu bastante, mas não o suficiente para ser um dos melhores modelos.\n",
        "\n",
        "Vamos analisar estes resultados graficamente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OZZiHl_Pb78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparação dos modelos com dados padronizados\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Comparação dos modelos com dados padronizados') \n",
        "ax = fig.add_subplot(111) \n",
        "pyplot.boxplot(results) \n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6c9Qy02J2Ms",
        "colab_type": "text"
      },
      "source": [
        "### 5.3. Ajuste dos Modelos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3S4Mf2QFOx",
        "colab_type": "text"
      },
      "source": [
        "#### Ajuste do KNN\n",
        "\n",
        "Podemos ajustar o número de vizinhos e as métricas de distância para o KNN. Tentaremos todos os valores ímpares de k de 1 a 21 e as métricas de distância euclideana, manhattan e minkowski. Cada valor de k e de distância será avaliado usando a validação cruzada 10-fold no conjunto de dados padronizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5F8jVyvJCZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tuning do KNN\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "\n",
        "k = [1,3,5,7,9,11,13,15,17,19,21]\n",
        "distancias = [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
        "param_grid = dict(n_neighbors=k, metric=distancias)\n",
        "\n",
        "model = KNeighborsRegressor()\n",
        "\n",
        "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold, iid=True)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "\n",
        "print(\"Melhor: %f usando %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) com: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZaCMvJiMfBZ",
        "colab_type": "text"
      },
      "source": [
        "Vimos que a melhor configuração tem k = 3 e distância de manhattan, fornecendo um erro quadrático médio de -15.076631, o melhor até agora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_bBSxCWJ9rI",
        "colab_type": "text"
      },
      "source": [
        "## 6. Métodos Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFMbOmlXR3Lm",
        "colab_type": "text"
      },
      "source": [
        "Outra maneira de melhorar o desempenho dos algoritmos é usar métodos de ensemble. Avaliaremos quatro modelos diferentes, sendo dois métodos de Boosting e dois de Bagging:\n",
        "* Métodos de Boosting: AdaBoost (AB) e Gradient Boosting (GBM).\n",
        "* Métodos de Bagging: Random Forests (RF) e Extra Trees (ET).\n",
        "\n",
        "Usaremos novamente a validação cruzada 10-fold e pipelines que irão padronizar os dados de treino para cada fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXUylqJ7J9y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensembles\n",
        "\n",
        "ensembles = []\n",
        "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\n",
        "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
        "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=10))])))\n",
        "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor(n_estimators=10))])))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "for name, model in ensembles:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoNeEu6bJ95t",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que em geral obtemos melhores pontuações do que nossos algoritmos lineares e não lineares nas seções anteriores.\n",
        "\n",
        "Vamos comparar os resultados graficamente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W91cAoxyJ9_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparação de modelos\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Comparação de modelos Ensemble com dados padronizados') \n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lN2-fz9Mnc3",
        "colab_type": "text"
      },
      "source": [
        "Os melhores resultados parecem ser dos modelos Gradient Boosting e Extra Trees. Provavelmente, podemos fazer melhor, já que as técnicas de ensemble usaram os parâmetros padrão. Em seguida, analisaremos o ajuste do Gradient Boosting para aumentar ainda mais o desempenho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K40TJiLfJ-Fr",
        "colab_type": "text"
      },
      "source": [
        "### 6.1. Ajuste dos métodos Ensemble\n",
        "\n",
        "O número padrão de estimadores é 100 e, frequentemente, quanto maior o número de estimadores, melhor o desempenho, porém maior o tempo de treinamento. Examinaremos o ajuste do número de estimadores, definindo valores entre 50 e 400, em incrementos de 50. Cada configuração será avaliada usando a validação cruzada 10-fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Z3w_owJ-La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajuste do GBM com dados padronizados\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "\n",
        "param_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\n",
        "model = GradientBoostingRegressor(random_state=seed)\n",
        "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold, iid=True)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Melhor: %f usando %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) com: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlyKmSgtMtVv",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que a melhor configuração foi com n_estimadores = 400, resultando em um erro quadrado médio de -9,353870, melhor que o método não ajustado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNiVJoDiU0wh",
        "colab_type": "text"
      },
      "source": [
        "## 7. Finalização do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cInEra-HKST9",
        "colab_type": "text"
      },
      "source": [
        "Neste estudo, o gradient boosting apresentou os melhores resultados. Finalizaremos o modelo treinando-o em todo o conjunto de dados de treinamento e faremos predições para o conjunto de dados de validação (separado anteriormente) para confirmar nossas descobertas. \n",
        "\n",
        "Iremos padronizar todo o conjunto de dados de treinamento e depois, aplicar a mesma transformação aos atributos de entrada do conjunto de dados de validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-DFCTVqKSbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparação do modelo\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "model = GradientBoostingRegressor(random_state=seed, n_estimators=400)\n",
        "model.fit(rescaledX, Y_train)\n",
        "\n",
        "# Estimativa do MSE no conjunto de validação\n",
        "rescaledValidationX = scaler.transform(X_validation)\n",
        "predictions = model.predict(rescaledValidationX)\n",
        "print(mean_squared_error(Y_validation, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9N3oVYwMx1d",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que o erro quadrático médio estimado é de 11,8, próximo à nossa estimativa anterior de -9,3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmbGP63iKSrj",
        "colab_type": "text"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Trabalhamos com um problema de aprendizado de máquina de modelagem preditiva de regressão de ponta a ponta. As etapas abordadas foram:\n",
        "\n",
        "* Definição do problema (Boston house price data).\n",
        "* Carga dos dados.\n",
        "* Análise dos dados (algumas distribuições distorcidas e atributos correlatos).\n",
        "* Avaliação de modelos (Linear Regression parecia bom).\n",
        "* Avaliação de modelos com padronização (KNN parecia bom).\n",
        "* Ajuste dos modelos (K=3 para KNN foi melhor).\n",
        "* Métodos de ensemble (Bagging and Boosting, Gradient Boosting parecia bom).\n",
        "* Ajuste dos métodos de Ensemble (melhorando o Gradient Boosting).\n",
        "* Finalização do modelo (use todos os dados de treinamento e valide usando o conjunto de dados de validação)."
      ]
    }
  ]
}